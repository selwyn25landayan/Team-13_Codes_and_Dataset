{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose estimation video saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "class Pose:\n",
    "    KEYPOINTS = (\n",
    "        'nose', 'left eye', 'right eye', 'left ear', 'right ear',\n",
    "        'left shoulder', 'right shoulder', 'left elbow', 'right elbow',\n",
    "        'left wrist', 'right wrist', 'left hip', 'right hip', 'left knee',\n",
    "        'right knee', 'left ankle', 'right ankle'\n",
    "    )\n",
    "\n",
    "    EDGES = (\n",
    "        ('nose', 'left eye'), ('nose', 'right eye'), ('nose', 'left ear'), \n",
    "        ('nose', 'right ear'), ('left ear', 'left eye'), ('right ear', 'right eye'), \n",
    "        ('left eye', 'right eye'), ('left shoulder', 'right shoulder'), \n",
    "        ('left shoulder', 'left elbow'), ('left shoulder', 'left hip'), \n",
    "        ('right shoulder', 'right elbow'), ('right shoulder', 'right hip'), \n",
    "        ('left elbow', 'left wrist'), ('right elbow', 'right wrist'), \n",
    "        ('left hip', 'right hip'), ('left hip', 'left knee'), ('right hip', 'right knee'), \n",
    "        ('left knee', 'left ankle'), ('right knee', 'right ankle')\n",
    "    )\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.tflite_interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.tflite_interpreter.allocate_tensors()\n",
    "        self.input_details = self.tflite_interpreter.get_input_details()\n",
    "        self.output_details = self.tflite_interpreter.get_output_details()\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def calc(self, img):\n",
    "        input_data = np.expand_dims(np.asarray(img).astype('float32') / 128.0 - 1.0, axis=0)\n",
    "        \n",
    "        # Ensure input_data is 4D (batch_size, height, width, channels)\n",
    "        if len(input_data.shape) == 3:\n",
    "            input_data = np.expand_dims(input_data, axis=-1)\n",
    "        \n",
    "        self.tflite_interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.tflite_interpreter.invoke()\n",
    "\n",
    "        output_tensor = [self.tflite_interpreter.get_tensor(self.output_details[i][\"index\"]) for i in range(len(self.output_details))]\n",
    "\n",
    "        heatmaps = np.asarray(output_tensor[0])\n",
    "        offsets = np.asarray(output_tensor[1])\n",
    "\n",
    "        height = heatmaps[0].shape[0]\n",
    "        width = heatmaps[0].shape[1]\n",
    "        numKeypoints = heatmaps[0][0][0].size\n",
    "\n",
    "        keypointPositions = []\n",
    "        for keypoint in range(numKeypoints):\n",
    "            maxVal = heatmaps[0][0][0][keypoint]\n",
    "            maxRow, maxCol = 0, 0\n",
    "            for row in range(height):\n",
    "                for col in range(width):\n",
    "                    if heatmaps[0][row][col][keypoint] > maxVal:\n",
    "                        maxVal = heatmaps[0][row][col][keypoint]\n",
    "                        maxRow, maxCol = row, col\n",
    "            keypointPositions.append((maxRow, maxCol))\n",
    "\n",
    "        output_dic = {}\n",
    "        total_score = 0.0\n",
    "        for idx, (bodypart, (positionY, positionX)) in enumerate(zip(Pose.KEYPOINTS, keypointPositions)):\n",
    "            output_dic[bodypart] = {\n",
    "                'x': int(positionX / (width - 1) * img.width + offsets[0][positionY][positionX][idx + numKeypoints]),\n",
    "                'y': int(positionY / (height - 1) * img.height + offsets[0][positionY][positionX][idx]),\n",
    "                'score': self._sigmoid(heatmaps[0][positionY][positionX][idx])\n",
    "            }\n",
    "            total_score += output_dic[bodypart]['score']\n",
    "        \n",
    "        output_dic['total_score'] = total_score / len(Pose.KEYPOINTS)\n",
    "\n",
    "        return output_dic\n",
    "\n",
    "    def draw_pose(self, pose, img, threshold=0.5, marker_color='green', color='yellow', marker_size=5, thickness=2):\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        for p1, p2 in Pose.EDGES:\n",
    "            if (pose[p1]['score'] < threshold) or (pose[p2]['score'] < threshold):\n",
    "                continue\n",
    "            draw.line((pose[p1]['x'], pose[p1]['y'], pose[p2]['x'], pose[p2]['y']), fill=color, width=thickness)\n",
    "\n",
    "        for label, keypoint in pose.items():\n",
    "            if label == 'total_score':\n",
    "                break\n",
    "            if keypoint['score'] < threshold:\n",
    "                continue\n",
    "            draw.ellipse(\n",
    "                (int(keypoint['x'] - marker_size / 2), int(keypoint['y'] - marker_size / 2),\n",
    "                 int(keypoint['x'] + marker_size / 2), int(keypoint['y'] + marker_size / 2)),\n",
    "                fill=marker_color)\n",
    "\n",
    "        return img\n",
    "\n",
    "# Initialize the Pose class with your model path\n",
    "pose = Pose(r\"C:\\Users\\Renzo\\Documents\\Project Design\\PoseNet\\posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite\")\n",
    "\n",
    "# Load the input video\n",
    "input_video_path = r'C:\\Users\\Renzo\\Documents\\Project Design\\Datasets\\test\\jumping_jacks\\jump jacks_39.mp4'\n",
    "output_video_path = r'C:\\Users\\Renzo\\Documents\\Project Design\\PoseNet\\Pose Output\\output_video.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to PIL Image\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((257, 257))\n",
    "\n",
    "    # Perform keypoint detection\n",
    "    output_dict = pose.calc(img)\n",
    "\n",
    "    # Draw pose on the image\n",
    "    img_with_pose = pose.draw_pose(output_dict, img, threshold=0.5, marker_color='green', color='yellow', marker_size=10, thickness=2)\n",
    "\n",
    "    # Convert the image with pose back to OpenCV format\n",
    "    frame_with_pose = cv2.cvtColor(np.array(img_with_pose.resize((frame_width, frame_height))), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    out.write(frame_with_pose)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Pose estimation video saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
